{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT8wGfuwhM09"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "import tensorflow as tfs\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D,LeakyReLU\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2, random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CABdsel4jI4C"
      },
      "source": [
        "height=32\n",
        "width=32\n",
        "depth=3\n",
        "\n",
        "inputShape = (height, width, depth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeP3bd4jhVb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f65a2a7b-7cb5-4ec0-e541-cf908315b938"
      },
      "source": [
        "# Prepare the train and test dataset.\n",
        "from tensorflow import keras\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, 32, 32, 3))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, 32, 32, 3))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169001437/169001437 [==============================] - 6s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "pA2C0sm6Pnme",
        "outputId": "165e382b-d481-46b6-96e2-a51be91aa298"
      },
      "source": [
        "from  keras.utils import np_utils\n",
        "# y_train = np_utils.to_categorical(y_train)\n",
        "# y_test= np_utils.to_categorical(y_test)\n",
        "# print(y_train[0])\n",
        "print(y_train.shape,y_test.shape)\n",
        "print(x_train.shape)\n",
        "# print(y_train[0],x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b78cbc0b9ee6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# y_train = np_utils.to_categorical(y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# y_test= np_utils.to_categorical(y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(y_train[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'np_utils' from 'keras.utils' (/usr/local/lib/python3.10/dist-packages/keras/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNobqP1lhXxZ"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "x_train, y_train= shuffle(x_train, y_train, random_state=22)\n",
        "X=[]\n",
        "Y=[]\n",
        "j=0\n",
        "for i in range(40):\n",
        "  X.append(x_train[j:j+2500])\n",
        "  Y.append(y_train[j:j+2500])\n",
        "  j+=1200\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqvga8B6CJxf"
      },
      "source": [
        "X=np.array(X)\n",
        "Y=np.array(Y)\n",
        "print(X.shape,Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaSZFbVvDEuc"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "class MODEL:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(input_shape=inputShape, kernel_size=(2, 2), padding='same', strides=(2, 2), filters=32))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\n",
        "        model.add(Conv2D(kernel_size=(2, 2), padding='same', strides=(2, 2), filters=64))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding='same'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(100, activation='softmax'))\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1DhadKvkzLx"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    model.compile(  optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "    # loss,acc=model.evaluate(x_test,y_test)\n",
        "    acc,loss=model.evaluate(x_test,y_test)\n",
        "    return acc, loss\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "\n",
        "    return avg_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQMGvLN7DiLY"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXKPs2nih4e2"
      },
      "source": [
        "global1= MODEL()\n",
        "global_model = global1.build()\n",
        "comms_round = 10\n",
        "# print(trainy[2].shape)\n",
        "epoch=5\n",
        "total_clusters=2\n",
        "total_clients=40\n",
        "clients_per_cluster=int(total_clients/total_clusters)\n",
        "X=np.reshape(X,(total_clusters,clients_per_cluster,2500,32,32,3))\n",
        "Y=np.reshape(Y,(total_clusters,clients_per_cluster,2500))\n",
        "print(X.shape,Y.shape)\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "for comm_round in range(comms_round):\n",
        "  global_weights = global_model.get_weights()\n",
        "  scaled_cluster_weight_list = list()\n",
        "  index=list({0,1,2,3,4})\n",
        "  #random.shuffle(index)\n",
        "  # print(index)\n",
        "  print(\"\\nCommunication Round:\" ,comm_round)\n",
        "  for clstr in range(total_clusters):\n",
        "    scaled_local_weight_list = list()\n",
        "    cluster_model=MODEL().build()\n",
        "    cluster_model.compile(optimizer=keras.optimizers.Adam(),\n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()],)\n",
        "\n",
        "    cluster_model.set_weights(global_weights)\n",
        "    cluster_weights = cluster_model.get_weights()\n",
        "    for ind in range(clients_per_cluster):\n",
        "\n",
        "      local = MODEL()\n",
        "      local_model=local.build()\n",
        "      local_model.compile(optimizer=keras.optimizers.Adam(),\n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()],)\n",
        "      local_model.set_weights(cluster_weights)\n",
        "\n",
        "      history=local_model.fit(X[clstr][ind],Y[clstr][ind], epochs=epoch,verbose=0)#,validation_data=(x_test,y_test))\n",
        "\n",
        "      scaling_factor=1.0/clients_per_cluster #1/no.ofclients\n",
        "      scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "      scaled_local_weight_list.append(scaled_weights)\n",
        "      K.clear_session()\n",
        "    average_cluster_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "    cluster_model.set_weights(average_cluster_weights)\n",
        "\n",
        "    scaling_cluster_factor=1.0/total_clusters #1/no.ofclients\n",
        "    scaled_cluster_weights = scale_model_weights(cluster_model.get_weights(), scaling_cluster_factor)\n",
        "    scaled_cluster_weight_list.append(scaled_cluster_weights)\n",
        "    K.clear_session()\n",
        "    cluster_loss, cluster_acc = test_model(x_test,y_test, cluster_model, comm_round)\n",
        "    print(\"CLUSTER \",clstr,\"ACC: \",cluster_acc,\"CLUSTER \",clstr,\"LOSS: \",cluster_loss)\n",
        "\n",
        "  average_weights = sum_scaled_weights(scaled_cluster_weight_list)\n",
        "  print(len(scaled_cluster_weight_list))\n",
        "  #global_model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "  global_model.set_weights(average_weights)\n",
        "  #val_acc,val_loss=global_model.evaluate(x_train,y_train)\n",
        "  #print(val_loss,val_acc)\n",
        "  global_loss, global_acc = test_model(x_test,y_test, global_model, comm_round)\n",
        "  print(\"GLOBAL ACCURACY\",global_acc,\"GLOBAL LOSS\",global_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyzYLNTGFPDs"
      },
      "source": [
        "print(global_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5hv8-pP3dpj"
      },
      "source": [
        "print(global_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5XQqHyYUyP8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}